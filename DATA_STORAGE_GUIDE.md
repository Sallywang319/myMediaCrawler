# 数据存储时机说明

## 问题描述

在AI辅助爬取系统中，数据存储发生在多个阶段。如果程序中断，可能只保存了部分数据。

## 数据存储时机

### 微博平台

#### 阶段1：搜索阶段（立即保存）
- **位置**：`ai_crawler.py` 的 `_search_weibo_and_collect_note_ids_with_crawler()` 方法
- **时机**：每搜索到一个note，立即保存
- **保存内容**：
  - 微博基本信息（note_id, content, 用户信息等）
  - 如果开启了媒体爬取，也会保存图片
- **特点**：此时保存的是搜索结果的摘要信息，可能不是完整文本

```python
# 在搜索阶段立即保存
await weibo_store.update_weibo_note(note_item)
```

#### 阶段2：相关性判断阶段
- **位置**：`ai_crawler.py` 的 `crawl_weibo()` 方法
- **时机**：对每个note进行相关性判断
- **保存内容**：无（只判断，不保存）
- **特点**：此阶段不保存数据，只进行判断

#### 阶段3：Detail模式（更新完整数据）
- **位置**：`ai_crawler.py` 的 `crawl_weibo()` 方法，detail模式执行时
- **时机**：对相关note执行detail模式爬取
- **保存内容**：
  - 完整的微博文本（search模式可能不完整）
  - 评论数据（如果开启了评论爬取）
  - 更新的统计数据
- **特点**：更新已保存的数据，获取完整信息

```python
# Detail模式会更新已保存的数据
config.CRAWLER_TYPE = "detail"
config.WEIBO_SPECIFIED_ID_LIST = relevant_note_ids
weibo_crawler_detail = WeiboCrawler()
await weibo_crawler_detail.start()
```

### B站平台

#### 阶段1：搜索阶段（自动保存）
- **位置**：`BilibiliCrawler.start()` → `search()` 方法
- **时机**：执行搜索时自动保存
- **保存内容**：视频基本信息
- **特点**：使用原始的爬虫逻辑，会自动保存

#### 阶段2：相关性判断阶段
- **位置**：`ai_crawler.py` 的 `crawl_bilibili()` 方法
- **时机**：对每个视频进行相关性判断
- **保存内容**：无（只判断，不保存）
- **特点**：此阶段不保存数据，只进行判断

### 知乎平台

#### 阶段1：搜索阶段（自动保存）
- **位置**：`ZhihuCrawler.start()` → `search()` 方法
- **时机**：执行搜索时自动保存
- **保存内容**：内容基本信息
- **特点**：使用原始的爬虫逻辑，会自动保存

#### 阶段2：相关性判断阶段
- **位置**：`ai_crawler.py` 的 `crawl_zhihu()` 方法
- **时机**：对每个内容进行相关性判断
- **保存内容**：无（只判断，不保存）
- **特点**：此阶段不保存数据，只进行判断

## 数据存储位置

根据 `config/base_config.py` 中的 `SAVE_DATA_OPTION` 配置：

- **json**：`data/{platform}/json/` 目录
- **csv**：`data/{platform}/csv/` 目录
- **sqlite**：SQLite数据库文件
- **db**：MySQL数据库

## 中断恢复策略

### 如果程序在搜索阶段中断
- **微博**：已保存的数据在 `data/weibo/` 目录下，但可能不完整
- **B站/知乎**：已保存的数据在对应目录下

### 如果程序在相关性判断阶段中断
- **所有平台**：搜索阶段的数据已保存，但可能包含不相关内容
- **建议**：可以手动过滤或重新运行程序

### 如果程序在Detail模式中断
- **微博**：搜索阶段的数据已保存，但可能缺少完整文本
- **建议**：可以手动运行detail模式获取完整数据

## 确保数据保存的建议

### 1. 使用数据库存储
数据库存储通常有事务机制，数据更安全：
```python
SAVE_DATA_OPTION = "sqlite"  # 或 "db"
```

### 2. 定期检查数据
在程序运行过程中，可以定期检查 `data/` 目录，确认数据正在保存。

### 3. 添加检查点机制
可以在关键步骤后添加检查点，确保数据已保存：
```python
# 示例：在保存后验证
await weibo_store.update_weibo_note(note_item)
# 可以添加验证逻辑，确认数据已保存
```

### 4. 使用日志监控
查看日志文件，确认数据保存操作是否成功：
```python
# 日志中会显示：
# [store.weibo.update_weibo_note] weibo note id:xxx, title:xxx ...
```

## 数据完整性说明

### 微博数据完整性
- **搜索阶段**：保存摘要信息，文本可能被截断
- **Detail阶段**：保存完整文本和详细信息
- **建议**：等待Detail模式完成，获取完整数据

### B站/知乎数据完整性
- **搜索阶段**：保存基本信息
- **通常足够**：搜索阶段的数据通常已包含主要信息

## 故障排查

### 问题：数据没有保存
1. 检查 `SAVE_DATA_OPTION` 配置是否正确
2. 检查 `data/` 目录是否有写入权限
3. 查看日志文件，确认是否有保存错误

### 问题：数据不完整
1. 微博：确保Detail模式已执行
2. 检查程序是否正常完成
3. 查看日志，确认是否有错误中断

### 问题：数据重复
1. 数据库存储有去重机制
2. JSON/CSV存储可能重复，需要手动去重

## 最佳实践

1. **使用数据库存储**：更安全，有去重机制
2. **等待程序完成**：确保Detail模式执行完成
3. **定期备份**：定期备份 `data/` 目录
4. **监控日志**：关注日志中的保存信息
5. **分批处理**：对于大量数据，可以分批处理，每批完成后检查

